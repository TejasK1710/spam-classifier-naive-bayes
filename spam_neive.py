# -*- coding: utf-8 -*-
"""spam-neive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jR1KBJjbzpuFWUAMRB_SANX0UKaWQ6ft

** Spam Classifier Using Naïve Bayes **
"""

import pandas as pd
import numpy as np

df = pd.read_csv('spam.csv',encoding="latin-1")
df.head()

# Keep only necessary columns
df = df[['v1', 'v2']]  # Selecting only required columns

# Rename columns for better readability
df.columns = ['label', 'message']

# Convert labels to binary (ham = 0, spam = 1)
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

# Save cleaned dataset
df.to_csv("cleaned_spam.csv", index=False)

print("✅ Data Preprocessing Complete. Cleaned file saved as cleaned_spam.csv")

df.head()

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load cleaned dataset
df = pd.read_csv("cleaned_spam.csv")

# Initialize Lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

# Function for text preprocessing
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\W', ' ', text)  # Remove special characters
    words = word_tokenize(text)  # Tokenization
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize
    return " ".join(words)

# Apply preprocessing
df["processed_message"] = df["message"].apply(preprocess_text)

# Save preprocessed data
df.to_csv("preprocessed_spam.csv", index=False)

print("✅ Text Preprocessing Complete. Data saved as preprocessed_spam.csv")

import nltk

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.tokenize import word_tokenize

text = "Hello, how are you?"
tokens = word_tokenize(text)
print(tokens)  # Should output: ['Hello', ',', 'how', 'are', 'you', '?']

import nltk
nltk.data.path.append('/root/nltk_data')

